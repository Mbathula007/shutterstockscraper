{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTyps7id4upZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as ec\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import ssl\n",
        "\n",
        "# For python2\n",
        "# from urllib import urlretrieve\n",
        "# import Tkinter, Tkconstants, tkFileDialog\n",
        "\n",
        "# def askDialog():\n",
        "   # return tkFileDialog.askdirectory()\n",
        "\n",
        "# def inp(text):\n",
        "    #return raw_input(text)\n",
        "\n",
        "# For python3\n",
        "from urllib.request import urlretrieve\n",
        "import tkinter, tkinter.constants, tkinter.filedialog\n",
        "\n",
        "def inp(text):\n",
        "    return input(text)\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "scrape_directory = \"/content/drive/My Drive/image_data/anger\"\n",
        "def videoscrape():\n",
        "    try:\n",
        "        driver = webdriver.Chrome()\n",
        "        driver.maximize_window()\n",
        "        for i in range(1, searchPage + 1):\n",
        "            url = \"https://www.shutterstock.com/video/search/\" + searchTerm + \"?page=\" + str(i)\n",
        "            driver.get(url)\n",
        "            print(\"Page \" + str(i))\n",
        "            for j in range (0, 50):\n",
        "                while True:\n",
        "                    container = driver.find_elements_by_xpath(\"//div[@data-automation='VideoGrid_video_videoClipPreview_\" + str(j) + \"']\")\n",
        "                    if len(container) != 0:\n",
        "                        break\n",
        "                    if len(driver.find_elements_by_xpath(\"//div[@data-automation='VideoGrid_video_videoClipPreview_\" + str(j + 1) + \"']\")) == 0 and i == searchPage:\n",
        "                        driver.close()\n",
        "                        return\n",
        "                    time.sleep(10)\n",
        "                    driver.get(url)\n",
        "                container[0].click()\n",
        "                while True:\n",
        "                    wait = WebDriverWait(driver, 60).until(ec.visibility_of_element_located((By.XPATH, \"//video[@data-automation='VideoPlayer_video_video']\")))\n",
        "                    video_url = driver.current_url\n",
        "                    data = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
        "                    scraper = BeautifulSoup(data, \"lxml\")\n",
        "                    video_container = scraper.find_all(\"video\", {\"data-automation\":\"VideoPlayer_video_video\"})\n",
        "                    if len(video_container) != 0:\n",
        "                        break\n",
        "                    time.sleep(10)\n",
        "                    driver.get(video_url)\n",
        "                video_array = video_container[0].find_all(\"source\")\n",
        "                video_src = video_array[1].get(\"src\")\n",
        "                name = video_src.rsplit(\"/\", 1)[-1]\n",
        "                try:\n",
        "                    urlretrieve(video_src, os.path.join(scrape_directory, os.path.basename(video_src)))\n",
        "                    print(\"Scraped \" + name)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                driver.get(url)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def imagescrape():\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options = webdriver.ChromeOptions()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "        driver.maximize_window()\n",
        "        for i in range(1, searchPage + 1):\n",
        "            url = \"https://www.shutterstock.com/search?searchterm=\" + searchTerm + \"&sort=popular&image_type=\" + image_type + \"&search_source=base_landing_page&language=en&page=\" + str(i)\n",
        "            driver.get(url)\n",
        "            data = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
        "            print(\"Page \" + str(i))\n",
        "            scraper = BeautifulSoup(data, \"lxml\")\n",
        "            K = scraper.find_all(\"a\")\n",
        "            img_container = []\n",
        "            for i in range(len(K)):\n",
        "                k = K[i].get(\"href\")\n",
        "                if k[1:6] == 'image':\n",
        "                    M = k.split(\"-\")\n",
        "                    L = \"\"\n",
        "                    for i in range(len(M)-1):\n",
        "                        L = L + str(\"-\") + str(M[i])\n",
        "                    u1 = str(\"https://image.shutterstock.com\") + str(L[1:])+str(\"-260nw-\") + str(M[-1])+str(\".jpg\")\n",
        "                    img_container.append(u1)\n",
        "            print(len(img_container))\n",
        "            for j in range(0, len(img_container)):\n",
        "                img_src = img_container[j]\n",
        "                name = img_src.rsplit(\"/\", 1)[-1]\n",
        "                print(name)\n",
        "                try:\n",
        "                    urlretrieve(img_src, os.path.join(scrape_directory, os.path.basename(img_src)))\n",
        "                    print(\"Scraped \" + name)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "        driver.close()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "print(\"ShutterScrape v1.1\")\n",
        "\n",
        "#scrape_directory = \"C:/Users/[username]/[path]\"\n",
        "\n",
        "while True:\n",
        "    while True:\n",
        "        if scrape_directory == None or scrape_directory == \"\":\n",
        "            print(\"You must select a directory to save your scraped files.\")\n",
        "            continue\n",
        "        break\n",
        "    while True:\n",
        "        searchMode = \"i\"\n",
        "        if searchMode != \"v\" and searchMode != \"i\":\n",
        "            print(\"You must select 'v' for video or 'i' for image.\")\n",
        "            continue\n",
        "        break\n",
        "    if searchMode == 'i':\n",
        "        while True:\n",
        "            image_type = \"a\"\n",
        "            if image_type != \"a\" and image_type != \"p\":\n",
        "                print(\"You must select 'a' for all or 'p' for photo.\")\n",
        "                continue\n",
        "            break\n",
        "        if image_type == 'p':\n",
        "            image_type = 'photo'\n",
        "        else:\n",
        "            image_type = 'all'\n",
        "    while True:\n",
        "        searchCount = 1\n",
        "        if searchCount < 1:\n",
        "            print(\"You must have at least one search term.\")\n",
        "            continue\n",
        "        elif searchCount == 1:\n",
        "            searchTerm = \"angry\"\n",
        "        else:\n",
        "            searchTerm = inp(\"Search term 1: \")\n",
        "            for i in range (1, searchCount):\n",
        "                searchTermPart = inp(\"Search term \" + str(i + 1) + \": \")\n",
        "                if searchMode == \"v\":\n",
        "                    searchTerm += \"-\" + searchTermPart\n",
        "                if searchMode == \"i\":\n",
        "                    searchTerm += \"+\" + searchTermPart\n",
        "        break\n",
        "    while True:\n",
        "        searchPage = int(10500)\n",
        "        if searchPage < 1:\n",
        "            print(\"You must have scrape at least one page.\")\n",
        "            continue\n",
        "        break\n",
        "    if searchMode == \"v\":\n",
        "        videoscrape()\n",
        "    if searchMode == \"i\":\n",
        "        imagescrape()\n",
        "    print(\"Scraping complete.\")\n",
        "    restartScrape = inp(\"Keep scraping? ('y' for yes or 'n' for no) \")\n",
        "    if restartScrape == \"n\":\n",
        "        print(\"Scraping ended.\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYUnx0OtMoYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "wd.get(\"https://www.webite-url.com\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}